{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Bucket Pruning — Optimizing Filtering on Bucketed Column (Reducing Bucket Files to Scan)\n",
    "### As of Spark 2.4, Spark SQL supports bucket pruning to optimize filtering on bucketed column (by reducing the number of bucket files to scan).\n",
    "\n",
    "### Bucket pruning supports the following predicate expressions:\n",
    "### \n",
    "### EqualTo (=)\n",
    "### \n",
    "### EqualNullSafe (<=>)\n",
    "### \n",
    "### In\n",
    "### \n",
    "### InSet\n",
    "### \n",
    "### And and Or of the above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst =[]\n",
    "for i in range(1000):\n",
    "    lst.append([str(i),\"name\"+str(i)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Bucket Pruining\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_in = sc.parallelize(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['0', 'name0'],\n",
       " ['1', 'name1'],\n",
       " ['2', 'name2'],\n",
       " ['3', 'name3'],\n",
       " ['4', 'name4']]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "rdd_in.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = spark.createDataFrame(rdd_in,[\"id\",\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  0| name0|\n|  1| name1|\n|  2| name2|\n|  3| name3|\n|  4| name4|\n|  5| name5|\n|  6| name6|\n|  7| name7|\n|  8| name8|\n|  9| name9|\n| 10|name10|\n| 11|name11|\n| 12|name12|\n| 13|name13|\n| 14|name14|\n| 15|name15|\n| 16|name16|\n| 17|name17|\n| 18|name18|\n| 19|name19|\n+---+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_in.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df_in.rdd.getNumPartitions()"
   ]
  },
  {
   "source": [
    "### Merging all files to single partition and then writing it to unbucket_demo1 table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in.coalesce(1).write.mode(\"overwrite\").saveAsTable(\"unbucket_demo1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in.coalesce(1).write.bucketBy(4,\"id\").sortBy(\"id\").mode(\"overwrite\").saveAsTable(\"bucket_demo1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_unbucket_table = spark.table(\"unbucket_demo1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Physical Plan ==\n*(1) Project [id#25, name#26]\n+- *(1) Filter (isnotnull(id#25) AND (id#25 = 1000))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet default.unbucket_demo1[id#25,name#26] Batched: true, DataFilters: [isnotnull(id#25), (id#25 = 1000)], Format: Parquet, Location: InMemoryFileIndex[file:/home/atif/PycharmProjects/test/spark_codes/spark_learn/spark-warehouse/un..., PartitionFilters: [], PushedFilters: [IsNotNull(id), EqualTo(id,1000)], ReadSchema: struct<id:string,name:string>\n\n\n"
     ]
    }
   ],
   "source": [
    "df_read_unbucket_table.where(\"id ='1000'\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "== Physical Plan ==\n*(1) Project [id#29, name#30]\n+- *(1) Filter (isnotnull(id#29) AND (id#29 = 1000))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet default.bucket_demo1[id#29,name#30] Batched: true, DataFilters: [isnotnull(id#29), (id#29 = 1000)], Format: Parquet, Location: InMemoryFileIndex[file:/home/atif/PycharmProjects/test/spark_codes/spark_learn/spark-warehouse/bu..., PartitionFilters: [], PushedFilters: [IsNotNull(id), EqualTo(id,1000)], ReadSchema: struct<id:string,name:string>, SelectedBucketsCount: 1 out of 4\n\n\n"
     ]
    }
   ],
   "source": [
    "df_read_bucket_table = spark.table(\"bucket_demo1\")\n",
    "df_read_bucket_table.where(\"id ='1000'\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|                  id|              string|   null|\n|                name|              string|   null|\n|                    |                    |       |\n|# Detailed Table ...|                    |       |\n|            Database|             default|       |\n|               Table|        bucket_demo1|       |\n|        Created Time|Wed Nov 18 22:52:...|       |\n|         Last Access|             UNKNOWN|       |\n|          Created By|         Spark 3.0.1|       |\n|                Type|             MANAGED|       |\n|            Provider|             parquet|       |\n|         Num Buckets|                   4|       |\n|      Bucket Columns|              [`id`]|       |\n|        Sort Columns|              [`id`]|       |\n|            Location|file:/home/atif/P...|       |\n+--------------------+--------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED bucket_demo1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+--------------------+-------+\n|            col_name|           data_type|comment|\n+--------------------+--------------------+-------+\n|                  id|              string|   null|\n|                name|              string|   null|\n|                    |                    |       |\n|# Detailed Table ...|                    |       |\n|            Database|             default|       |\n|               Table|      unbucket_demo1|       |\n|        Created Time|Wed Nov 18 22:50:...|       |\n|         Last Access|             UNKNOWN|       |\n|          Created By|         Spark 3.0.1|       |\n|                Type|             MANAGED|       |\n|            Provider|             parquet|       |\n|            Location|file:/home/atif/P...|       |\n+--------------------+--------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED unbucket_demo1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------------------------------------------------------------------------+\n|createtab_stmt                                                                          |\n+----------------------------------------------------------------------------------------+\n|CREATE TABLE `default`.`UNBUCKET_DEMO1` (\n  `id` STRING,\n  `name` STRING)\nUSING parquet\n|\n+----------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE UNBUCKET_DEMO1\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------+\n|createtab_stmt                                                                                                                        |\n+--------------------------------------------------------------------------------------------------------------------------------------+\n|CREATE TABLE `default`.`BUCKET_DEMO1` (\n  `id` STRING,\n  `name` STRING)\nUSING parquet\nCLUSTERED BY (id)\nSORTED BY (id)\nINTO 4 BUCKETS\n|\n+--------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CREATE TABLE BUCKET_DEMO1\").show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}