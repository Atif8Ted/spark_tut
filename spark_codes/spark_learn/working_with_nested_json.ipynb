{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "json_tuple_example.csv\tmarks.csv  marks_without_header.csv  weatherHistory.csv\n"
     ]
    }
   ],
   "source": [
    "!ls *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Json Tuple\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PartitionDate,Status,request\n2020-06-30,Internal Error,\"{\"\"Response\"\":{\"\"MessageId\"\":15432}}\"\n2020-06-30,Success,\"{\"\"Response\"\":{\"\"MessageId\"\":15432,\"\"Lattitude\"\":\"\"-176.2989\"\",\"\"Longitude\"\":\"\"7.3614\"\"}}\"\n"
     ]
    }
   ],
   "source": [
    "! cat json_tuple_example.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_csv = spark.read.csv(\"json_tuple_example.csv\",multiLine=True,escape=\"\\\"\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+--------------+-----------------------------------------------------------------------------+\n|PartitionDate|Status        |request                                                                      |\n+-------------+--------------+-----------------------------------------------------------------------------+\n|2020-06-30   |Internal Error|{\"Response\":{\"MessageId\":15432}}                                             |\n|2020-06-30   |Success       |{\"Response\":{\"MessageId\":15432,\"Lattitude\":\"-176.2989\",\"Longitude\":\"7.3614\"}}|\n+-------------+--------------+-----------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_json_csv.show(truncate=0)"
   ]
  },
  {
   "source": [
    "### Method 1 : Using JSON Tuple"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,json_tuple,to_json,from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+--------------+----------------------------------------------------------------+\n|PartitionDate|Status        |c0                                                              |\n+-------------+--------------+----------------------------------------------------------------+\n|2020-06-30   |Internal Error|{\"MessageId\":15432}                                             |\n|2020-06-30   |Success       |{\"MessageId\":15432,\"Lattitude\":\"-176.2989\",\"Longitude\":\"7.3614\"}|\n+-------------+--------------+----------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_json_level_1 = df_json_csv.select(\"*\",json_tuple(\"request\",\"Response\")).drop(\"request\")\n",
    "df_json_level_1.show(truncate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_level_2 = df_json_level_1.select(\"*\",json_tuple(\"c0\",\"MessageId\",\"Lattitude\",\"Longitude\").alias(\"MessageID\",\"Lattitude\",\"Longitude\")).drop(\"c0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+--------------+---------+---------+---------+\n|PartitionDate|        Status|MessageID|Lattitude|Longitude|\n+-------------+--------------+---------+---------+---------+\n|   2020-06-30|Internal Error|    15432|     null|     null|\n|   2020-06-30|       Success|    15432|-176.2989|   7.3614|\n+-------------+--------------+---------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_json_level_2.show()"
   ]
  },
  {
   "source": [
    "## Metjod 2 : from_json  :; Prefereable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['{\"Response\":{\"MessageId\":15432}}',\n",
       " '{\"Response\":{\"MessageId\":15432,\"Lattitude\":\"-176.2989\",\"Longitude\":\"7.3614\"}}']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df_json_csv.select(col('request').alias(\"jsoncol\")).rdd.map(lambda x:x.jsoncol).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}