{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=k4XXJ3Be_iE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Load_from_multiple_directory').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28\r\n",
      "-rw-r--r-- 1 atif atif   56 Nov 14 17:19  multi_deli_file.txt\r\n",
      "-rw-r--r-- 1 atif atif 7648 Nov 14 18:04  multi_delimeted_file_handling.ipynb\r\n",
      "-rw-r--r-- 1 atif atif 1043 Nov 14 18:07 'Load Data from multiple directory.ipynb'\r\n",
      "drwxr-xr-x 2 atif atif 4096 Nov 14 18:09  data2\r\n",
      "drwxr-xr-x 2 atif atif 4096 Nov 14 18:10  data3\r\n",
      "drwxr-xr-x 2 atif atif 4096 Nov 14 18:10  data1\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  have to read from data1 and data2 ignoring data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass * and run but since data3 schema is different it will , show some info at end of DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_multi_file = spark.read.csv('data*/test*.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130|   10|    5|     6|   280|      25|    3|     1|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|    2|    8|     8|   135|       0|    3|     1|   1|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|    9|    7|     5|   320|      25|    3|     1|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140|   14|    8|     0|   330|      25|    3|     1| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|    1|   14|     8|    -1|      25|    3|     1|0.75|34.384843|\n",
      "|Apple Cinnamon Ch...|  G|   C|     110|      2|  2|   180|  1.5| 10.5|    10|    70|      25|    1|     1|0.75|29.509541|\n",
      "|         Apple Jacks|  K|   C|     110|      2|  0|   125|    1|   11|    14|    30|      25|    2|     1|   1|33.174094|\n",
      "|             Basic 4|  G|   C|     130|      3|  2|   210|    2|   18|     8|   100|      25|    3|  1.33|0.75|37.038562|\n",
      "|           Bran Chex|  R|   C|      90|      2|  1|   200|    4|   15|     6|   125|      25|    1|     1|0.67|49.120253|\n",
      "|         Bran Flakes|  P|   C|      90|      3|  0|   210|    5|   13|     5|   190|      25|    3|     1|0.67|53.313813|\n",
      "|        Cap'n'Crunch|  Q|   C|     120|      1|  2|   220|    0|   12|    12|    35|      25|    2|     1|0.75|18.042851|\n",
      "|            Cheerios|  G|   C|     110|      6|  2|   290|    2|   17|     1|   105|      25|    1|     1|1.25|50.764999|\n",
      "|Cinnamon Toast Cr...|  G|   C|     120|      1|  3|   210|    0|   13|     9|    45|      25|    2|     1|0.75|19.823573|\n",
      "|            Clusters|  G|   C|     110|      3|  2|   140|    2|   13|     7|   105|      25|    3|     1| 0.5|40.400208|\n",
      "|         Cocoa Puffs|  G|   C|     110|      1|  1|   180|    0|   12|    13|    55|      25|    2|     1|   1|22.736446|\n",
      "|           Corn Chex|  R|   C|     110|      2|  0|   280|    0|   22|     3|    25|      25|    1|     1|   1|41.445019|\n",
      "|         Corn Flakes|  K|   C|     100|      2|  0|   290|    1|   21|     2|    35|      25|    1|     1|   1|45.863324|\n",
      "|           Corn Pops|  K|   C|     110|      1|  0|    90|    1|   13|    12|    20|      25|    2|     1|   1|35.782791|\n",
      "|       Count Chocula|  G|   C|     110|      1|  1|   180|    0|   12|    13|    65|      25|    2|     1|   1|22.396513|\n",
      "|  Cracklin' Oat Bran|  K|   C|     110|      3|  3|   140|    4|   10|     7|   160|      25|    3|     1| 0.5|40.448772|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multi_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the DF got messed up because of different file schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- mfr: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- calories: string (nullable = true)\n",
      " |-- protein: string (nullable = true)\n",
      " |-- fat: string (nullable = true)\n",
      " |-- sodium: string (nullable = true)\n",
      " |-- fiber: string (nullable = true)\n",
      " |-- carbo: string (nullable = true)\n",
      " |-- sugars: string (nullable = true)\n",
      " |-- potass: string (nullable = true)\n",
      " |-- vitamins: string (nullable = true)\n",
      " |-- shelf: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      " |-- cups: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multi_file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so, to overcome this , we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 : list of input path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path_list =['data1/*.csv','data2/*.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_input =spark.read.csv(input_path_list,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|age|\n",
      "+------+---+\n",
      "|  Atif| 25|\n",
      "|kashif| 26|\n",
      "|  asif| 27|\n",
      "|  imam| 26|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 : Using regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_input = spark.read.csv('data[1-2]*/*.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|age|\n",
      "+------+---+\n",
      "|  Atif| 25|\n",
      "|kashif| 26|\n",
      "|  asif| 27|\n",
      "|  imam| 26|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_reg_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3 : curly braces with selected items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_curl_input = spark.read.csv('data{1,2}*/*.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|age|\n",
      "+------+---+\n",
      "|  Atif| 25|\n",
      "|kashif| 26|\n",
      "|  asif| 27|\n",
      "|  imam| 26|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_curl_input.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
